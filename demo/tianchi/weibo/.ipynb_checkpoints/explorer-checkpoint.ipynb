{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# notebook概述：\n",
    "- 第一部分：大致了解数据内容\n",
    "- 第二部分: 比较几种工具的中文分词效果\n",
    "- 第三部分：分词的分布统计探索"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一部分："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pdb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "PATH = 'F:/codeGit/dataset/weibo/'\n",
    "\n",
    "FILE_TRAIN = 'weibo_train_data.txt'\n",
    "FILE_PREDICT = 'weibo_predict_data.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readTxtToDataframe(path_dataset, filename):\n",
    "    const = open(path_dataset + filename).readlines()\n",
    "    items = []\n",
    "    \n",
    "    for line in const:\n",
    "        item = []\n",
    "        elems = line.replace('\\n', '').split('\\t')\n",
    "        elems[3] = int(elems[3])\n",
    "        elems[4] = int(elems[3])\n",
    "        elems[5] = int(elems[3])\n",
    "        elems[6] = ''.join(elems[6: ])\n",
    "        items.append(elems[:7])\n",
    "        # pdb.set_trace()\n",
    "    cols_name = ['uid', 'mid', 'time', 'forward_cout', 'comment_count', 'like_count', 'content']\n",
    "    return DataFrame(items, columns = cols_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                uid                               mid  \\\n",
      "0  d38e9bed5d98110dc2489d0d1cac3c2a  7d45833d9865727a88b960b0603c19f6   \n",
      "1  fa13974743d3fe6ff40d21b872325e9e  8169f1d45051e08ef213bf1106b1225d   \n",
      "2  da534fe87e7a52777bee5c30573ed5fd  68cd0258c31c2c525f94febea2d9523b   \n",
      "3  e06a22b7e065e559a1f0bf7841a85c51  00b9f86b4915aedb7db943c54fd19d59   \n",
      "4  f9828598f9664d4e347ef2048ce17734  c7f6f66044c0c5a3330e2c5371be6824   \n",
      "\n",
      "                  time  forward_cout  comment_count  like_count  \\\n",
      "0  2015-02-23 17:41:29             0              0           0   \n",
      "1  2015-02-14 12:49:58             0              0           0   \n",
      "2  2015-03-31 13:58:06             0              0           0   \n",
      "3  2015-06-11 20:39:57             0              0           0   \n",
      "4  2015-03-10 18:02:38             0              0           0   \n",
      "\n",
      "                                             content  \n",
      "0  丽江旅游(sz002033)#股票##炒股##财经##理财##投资#推荐包赢股，盈利对半分成...  \n",
      "1  #丁辰灵的红包#挣钱是一种能力，抢红包拼的是技术。我抢到了丁辰灵 和@阚洪岩 一起发出的现金...  \n",
      "2                      淘宝网这些傻逼。。。气的劳资有火没地儿发~尼玛，你们都瞎了  \n",
      "3                                  看点不能说的，你们都懂[笑cry]  \n",
      "4                                              111多张  \n"
     ]
    }
   ],
   "source": [
    "df = readTxtToDataframe(PATH, FILE_TRAIN)\n",
    "print df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二部分："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba as jb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package jieba:\n",
      "\n",
      "NAME\n",
      "    jieba\n",
      "\n",
      "FILE\n",
      "    d:\\anaconda2\\lib\\site-packages\\jieba\\__init__.py\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    __main__\n",
      "    _compat\n",
      "    analyse (package)\n",
      "    finalseg (package)\n",
      "    posseg (package)\n",
      "\n",
      "CLASSES\n",
      "    __builtin__.object\n",
      "        Tokenizer\n",
      "    \n",
      "    class Tokenizer(__builtin__.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, dictionary=None)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  add_word(self, word, freq=None, tag=None)\n",
      "     |      Add a word to dictionary.\n",
      "     |      \n",
      "     |      freq and tag can be omitted, freq defaults to be a calculated value\n",
      "     |      that ensures the word can be cut out.\n",
      "     |  \n",
      "     |  calc(self, sentence, DAG, route)\n",
      "     |  \n",
      "     |  check_initialized(self)\n",
      "     |  \n",
      "     |  cut(self, sentence, cut_all=False, HMM=True)\n",
      "     |      The main function that segments an entire sentence that contains\n",
      "     |      Chinese characters into seperated words.\n",
      "     |      \n",
      "     |      Parameter:\n",
      "     |          - sentence: The str(unicode) to be segmented.\n",
      "     |          - cut_all: Model type. True for full pattern, False for accurate pattern.\n",
      "     |          - HMM: Whether to use the Hidden Markov Model.\n",
      "     |  \n",
      "     |  cut_for_search(self, sentence, HMM=True)\n",
      "     |      Finer segmentation for search engines.\n",
      "     |  \n",
      "     |  del_word(self, word)\n",
      "     |      Convenient function for deleting a word.\n",
      "     |  \n",
      "     |  gen_pfdict(self, f)\n",
      "     |  \n",
      "     |  get_DAG(self, sentence)\n",
      "     |  \n",
      "     |  get_dict_file(self)\n",
      "     |  \n",
      "     |  initialize(self, dictionary=None)\n",
      "     |  \n",
      "     |  lcut(self, *args, **kwargs)\n",
      "     |  \n",
      "     |  lcut_for_search(self, *args, **kwargs)\n",
      "     |  \n",
      "     |  load_userdict(self, f)\n",
      "     |      Load personalized dict to improve detect rate.\n",
      "     |      \n",
      "     |      Parameter:\n",
      "     |          - f : A plain text file contains words and their ocurrences.\n",
      "     |                Can be a file-like object, or the path of the dictionary file,\n",
      "     |                whose encoding must be utf-8.\n",
      "     |      \n",
      "     |      Structure of dict file:\n",
      "     |      word1 freq1 word_type1\n",
      "     |      word2 freq2 word_type2\n",
      "     |      ...\n",
      "     |      Word type may be ignored\n",
      "     |  \n",
      "     |  set_dictionary(self, dictionary_path)\n",
      "     |  \n",
      "     |  suggest_freq(self, segment, tune=False)\n",
      "     |      Suggest word frequency to force the characters in a word to be\n",
      "     |      joined or splitted.\n",
      "     |      \n",
      "     |      Parameter:\n",
      "     |          - segment : The segments that the word is expected to be cut into,\n",
      "     |                      If the word should be treated as a whole, use a str.\n",
      "     |          - tune : If True, tune the word frequency.\n",
      "     |      \n",
      "     |      Note that HMM may affect the final result. If the result doesn't change,\n",
      "     |      set HMM=False.\n",
      "     |  \n",
      "     |  tokenize(self, unicode_sentence, mode=u'default', HMM=True)\n",
      "     |      Tokenize a sentence and yields tuples of (word, start, end)\n",
      "     |      \n",
      "     |      Parameter:\n",
      "     |          - sentence: the str(unicode) to be segmented.\n",
      "     |          - mode: \"default\" or \"search\", \"search\" is for finer segmentation.\n",
      "     |          - HMM: whether to use the Hidden Markov Model.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "FUNCTIONS\n",
      "    add_word(self, word, freq=None, tag=None) method of Tokenizer instance\n",
      "        Add a word to dictionary.\n",
      "        \n",
      "        freq and tag can be omitted, freq defaults to be a calculated value\n",
      "        that ensures the word can be cut out.\n",
      "    \n",
      "    calc(self, sentence, DAG, route) method of Tokenizer instance\n",
      "    \n",
      "    cut(self, sentence, cut_all=False, HMM=True) method of Tokenizer instance\n",
      "        The main function that segments an entire sentence that contains\n",
      "        Chinese characters into seperated words.\n",
      "        \n",
      "        Parameter:\n",
      "            - sentence: The str(unicode) to be segmented.\n",
      "            - cut_all: Model type. True for full pattern, False for accurate pattern.\n",
      "            - HMM: Whether to use the Hidden Markov Model.\n",
      "    \n",
      "    cut_for_search(self, sentence, HMM=True) method of Tokenizer instance\n",
      "        Finer segmentation for search engines.\n",
      "    \n",
      "    del_word(self, word) method of Tokenizer instance\n",
      "        Convenient function for deleting a word.\n",
      "    \n",
      "    disable_parallel()\n",
      "    \n",
      "    enable_parallel(processnum=None)\n",
      "        Change the module's `cut` and `cut_for_search` functions to the\n",
      "        parallel version.\n",
      "        \n",
      "        Note that this only works using dt, custom Tokenizer\n",
      "        instances are not supported.\n",
      "    \n",
      "    get_DAG(self, sentence) method of Tokenizer instance\n",
      "    \n",
      "    get_FREQ lambda k, d=None\n",
      "    \n",
      "    get_dict_file(self) method of Tokenizer instance\n",
      "    \n",
      "    initialize(self, dictionary=None) method of Tokenizer instance\n",
      "    \n",
      "    lcut(self, *args, **kwargs) method of Tokenizer instance\n",
      "    \n",
      "    lcut_for_search(self, *args, **kwargs) method of Tokenizer instance\n",
      "    \n",
      "    load_userdict(self, f) method of Tokenizer instance\n",
      "        Load personalized dict to improve detect rate.\n",
      "        \n",
      "        Parameter:\n",
      "            - f : A plain text file contains words and their ocurrences.\n",
      "                  Can be a file-like object, or the path of the dictionary file,\n",
      "                  whose encoding must be utf-8.\n",
      "        \n",
      "        Structure of dict file:\n",
      "        word1 freq1 word_type1\n",
      "        word2 freq2 word_type2\n",
      "        ...\n",
      "        Word type may be ignored\n",
      "    \n",
      "    log(...)\n",
      "        log(x[, base])\n",
      "        \n",
      "        Return the logarithm of x to the given base.\n",
      "        If the base not specified, returns the natural logarithm (base e) of x.\n",
      "    \n",
      "    md5 = openssl_md5(...)\n",
      "        Returns a md5 hash object; optionally initialized with a string\n",
      "    \n",
      "    setLogLevel(log_level)\n",
      "    \n",
      "    set_dictionary(self, dictionary_path) method of Tokenizer instance\n",
      "    \n",
      "    suggest_freq(self, segment, tune=False) method of Tokenizer instance\n",
      "        Suggest word frequency to force the characters in a word to be\n",
      "        joined or splitted.\n",
      "        \n",
      "        Parameter:\n",
      "            - segment : The segments that the word is expected to be cut into,\n",
      "                        If the word should be treated as a whole, use a str.\n",
      "            - tune : If True, tune the word frequency.\n",
      "        \n",
      "        Note that HMM may affect the final result. If the result doesn't change,\n",
      "        set HMM=False.\n",
      "    \n",
      "    tokenize(self, unicode_sentence, mode=u'default', HMM=True) method of Tokenizer instance\n",
      "        Tokenize a sentence and yields tuples of (word, start, end)\n",
      "        \n",
      "        Parameter:\n",
      "            - sentence: the str(unicode) to be segmented.\n",
      "            - mode: \"default\" or \"search\", \"search\" is for finer segmentation.\n",
      "            - HMM: whether to use the Hidden Markov Model.\n",
      "\n",
      "DATA\n",
      "    DEFAULT_DICT = None\n",
      "    DEFAULT_DICT_NAME = u'dict.txt'\n",
      "    DICT_WRITING = {}\n",
      "    PY2 = True\n",
      "    __license__ = u'MIT'\n",
      "    __version__ = u'0.38'\n",
      "    absolute_import = _Feature((2, 5, 0, 'alpha', 1), (3, 0, 0, 'alpha', 0...\n",
      "    default_encoding = 'mbcs'\n",
      "    default_logger = <logging.Logger object>\n",
      "    dt = <Tokenizer dictionary=None>\n",
      "    log_console = <logging.StreamHandler object>\n",
      "    pool = None\n",
      "    re_eng = <_sre.SRE_Pattern object>\n",
      "    re_han_cut_all = <_sre.SRE_Pattern object>\n",
      "    re_han_default = <_sre.SRE_Pattern object>\n",
      "    re_skip_cut_all = <_sre.SRE_Pattern object>\n",
      "    re_skip_default = <_sre.SRE_Pattern object>\n",
      "    re_userdict = <_sre.SRE_Pattern object>\n",
      "    string_types = (<type 'str'>, <type 'unicode'>)\n",
      "    unicode_literals = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', ...\n",
      "    user_word_tag_tab = {}\n",
      "\n",
      "VERSION\n",
      "    0.38\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(jb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "淘宝网/这些/傻/逼/。/。/。/气/的/劳资/有火/没/地儿/发/~/尼玛/，/你们/都/瞎/了\n",
      "淘宝/宝网/淘宝网/这些/傻/逼/。/。/。/气/的/劳资/有火/没/地儿/发/~/尼玛/，/你们/都/瞎/了\n"
     ]
    }
   ],
   "source": [
    "seg_list1 = jb.cut(df.iloc[2]['content'], cut_all = False)\n",
    "seg_list2 = jb.cut_for_search(df.iloc[2]['content'])\n",
    "print \"/\".join(seg_list1)\n",
    "print \"/\".join(seg_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 比较文本方法的速度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = []\n",
    "t1 = time.time()\n",
    "for i in range(100000):\n",
    "    seg_list1 = jb.cut(df.iloc[i]['content'], cut_all = False)\n",
    "    words.extend(\"/\".join(seg_list1).split('/'))\n",
    "print time.time() - t1\n",
    "\n",
    "strs = ''\n",
    "t1 = time.time()\n",
    "for i in range(100000):\n",
    "    strs = strs + df.iloc[i]['content'] + ' '\n",
    "    seg_list = jb.cut(strs, cut_all = False)\n",
    "    words = \"/\".join(seg_list1).split('/')\n",
    "print time.time() - t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一种方法跑了40s左右，第二种方法跑了5分钟都没出结果。故采用第一种方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第三部分："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  word  cnt\n",
      "0       386\n",
      "1       195\n",
      "2    \"    2\n",
      "3    #   55\n",
      "4   ##    4\n"
     ]
    }
   ],
   "source": [
    "nSample = 10000\n",
    "words = []\n",
    "blog_idxs = []\n",
    "for i in range(nSample):\n",
    "    seg_list1 = jb.cut(df.iloc[i]['content'], cut_all = False)\n",
    "    words_line = \"/\".join(seg_list1).split('/')\n",
    "    blog_idxs.extend([df.index[i] for j in range(len(words_line))])\n",
    "    words.extend(words_line)\n",
    "\n",
    "df_words = DataFrame(words, columns = ['word'])\n",
    "df_words['blog_idx'] = Series(blog_idxs, index = df_words.index)\n",
    "\n",
    "df_cnt = x.groupby(['word']).count().reset_index()\n",
    "df_cnt.columns = ['word', 'cnt']\n",
    "print df_cnt[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    从上面的预览可以发现很多无意义的符号统计是最多的，而且这些符号有些并没有被切分出来，故需要在切割的时候为字典指定这些特殊符号。其次含有很多这些无意义的符号博文，大多数情况为一些诸如广告、推广文等没啥干货的垃圾博文，这点可以注意到。  \n",
    "**故可以利用这些无意义的符号出现次数来筛选掉那些无人关注的垃圾博文**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        word  cnt\n",
      "3          #   55\n",
      "4         ##    4\n",
      "7          (   12\n",
      "8          )   12\n",
      "11         ,   12\n",
      "12         -    9\n",
      "13         .   78\n",
      "14       ...    6\n",
      "43         3    6\n",
      "47         4    5\n",
      "55         6    5\n",
      "60         :   68\n",
      "61         ;    5\n",
      "64         @   37\n",
      "187        [   22\n",
      "188        ]   22\n",
      "189        _    5\n",
      "197       cn   63\n",
      "210     http   64\n",
      "212   iPhone    4\n",
      "236        t   63\n",
      "246        ~    9\n",
      "276        —    4\n",
      "277        “   14\n",
      "278        ”   12\n",
      "279        …    7\n",
      "280        ╭    4\n",
      "281        ╮    4\n",
      "283        、    9\n",
      "284        。   67\n",
      "...      ...  ...\n",
      "1297      矢量    4\n",
      "1299      知道    5\n",
      "1327       等    4\n",
      "1347      红包   14\n",
      "1367      网易    5\n",
      "1373      羊年    4\n",
      "1375      美元    5\n",
      "1390      而且    4\n",
      "1391      而是    5\n",
      "1401       能   10\n",
      "1402      能力    5\n",
      "1405       自    6\n",
      "1407      自己    6\n",
      "1417      苹果    4\n",
      "1444      觉得    4\n",
      "1468      试试    4\n",
      "1472       说    4\n",
      "1473       请    4\n",
      "1515       还    4\n",
      "1518       这    7\n",
      "1521      这些    5\n",
      "1544       都   22\n",
      "1583      音乐    7\n",
      "1617       �    6\n",
      "1622       ！   33\n",
      "1623       （   17\n",
      "1624       ）   16\n",
      "1626       ：   17\n",
      "1628       ？   11\n",
      "1629       ￣    8\n",
      "\n",
      "[138 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "cnt_up = int(df_cnt['cnt'].max() * 30 / 100)\n",
    "cnt_down = int(df_cnt['cnt'].max() * 1 / 100)\n",
    "print df_cnt[(df_cnt.cnt < cnt_up) & (df_cnt.cnt > cnt_down)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jb.load_userdict(PATH + 'rubbish.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  word  cnt\n",
      "0       386\n",
      "1       195\n",
      "2    \"    2\n",
      "3    #   55\n",
      "4   ##    4\n"
     ]
    }
   ],
   "source": [
    "nSample = 10000\n",
    "words = []\n",
    "blog_idxs = []\n",
    "for i in range(nSample):\n",
    "    seg_list1 = jb.cut(df.iloc[i]['content'], cut_all = False)\n",
    "    words_line = \"/\".join(seg_list1).split('/')\n",
    "    blog_idxs.extend([df.index[i] for j in range(len(words_line))])\n",
    "    words.extend(words_line)\n",
    "\n",
    "df_words = DataFrame(words, columns = ['word'])\n",
    "df_words['blog_idx'] = Series(blog_idxs, index = df_words.index)\n",
    "\n",
    "df_cnt = x.groupby(['word']).count().reset_index()\n",
    "df_cnt.columns = ['word', 'cnt']\n",
    "print df_cnt[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
