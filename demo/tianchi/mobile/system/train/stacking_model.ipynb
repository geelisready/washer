{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda2\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "# -------- import basic package --------\n",
    "import pdb\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import datetime as dt\n",
    "import warnings\n",
    "import copy\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import washer.utils.path.pathHandle as ph\n",
    "import washer.utils.store as store\n",
    "from washer.sample.feature import FeatSampler\n",
    "from washer.repairer.isolatedRepairer import GeneralRepairer\n",
    "\n",
    "from washer.demo.tianchi.mobile.system.utils.preProcess import *\n",
    "from washer.demo.tianchi.mobile.system.utils.selectFeature import *\n",
    "\n",
    "PATH_OFFLINE = \"F:/codeGit/my project/python/dataset/tianchi/offline/\"\n",
    "PATH_ONLINE = \"F:/codeGit/my project/python/dataset/tianchi/online/\"\n",
    "PATH_OF_DATAOUT = \"F:/codeGit/my project/python/dataset/tianchi/submission/\"\n",
    "PATH_MODEL = \"F:/codeGit/my project/python/dataset/tianchi/model/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getOffsetDate(date, offset):\n",
    "    if offset > 0:\n",
    "        date_next = (dt.datetime.strptime(date, '%Y-%m-%d') + dt.timedelta(offset)).strftime('%Y-%m-%d')\n",
    "    elif offset < 0:\n",
    "        date_next = (dt.datetime.strptime(date, '%Y-%m-%d') - dt.timedelta(offset * -1)).strftime('%Y-%m-%d')    \n",
    "    return date_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updateParasList(parasList, para, k):\n",
    "    if len(parasList) < k:\n",
    "        parasList.append(para)\n",
    "        minIdx = -1\n",
    "    else:\n",
    "        minIdx = Series(parasList).idxmin()\n",
    "        if para > parasList[minIdx]:\n",
    "            parasList[minIdx] = para\n",
    "    return parasList, minIdx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updateKModelList(kModelList, idx, model, k):\n",
    "    if len(kModelList) < k:\n",
    "        kModelList.append(model)\n",
    "        kModelList[idx] = model\n",
    "    return kModelList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainWithMutliTime(model, df, nTime, k):\n",
    "    print(\"trainig model...\")\n",
    "    parasList = []\n",
    "    kModelList = []\n",
    "\n",
    "    for i in range(nTime):\n",
    "        df_train, df_cv = selectSamplesByBagging(df, 0.3, 1)\n",
    "        X_test, y_test, le = transform_dataset(df_cv.drop(['user_id', 'item_id', 'item_category'], axis = 1))\n",
    "        X_train, y_train, le1 = transform_dataset(df_train.drop(['user_id', 'item_id', 'item_category'], axis = 1))\n",
    "        \n",
    "        paras = []\n",
    "        model_new = copy.deepcopy(model)\n",
    "        y_pred = model_new.fit(X_train, y_train).predict(X_test)\n",
    "        pred_labels = le.inverse_transform(y_pred)\n",
    "        F1, P ,R = calc_F1(y_test, pred_labels)\n",
    "        paras.append(F1)\n",
    "        paras.append(P)\n",
    "        paras.append(R)\n",
    "        parasList, index = updateParasList(parasList, paras[0], k)\n",
    "        kModelList = updateKModelList(kModelList, index, model_new, k)\n",
    "        # print 'mean: F1/P/R %.2f%%/%.2f%%/%.2f%%\\n' %(paras[0], paras[1], paras[2])\n",
    "    return kModelList, parasList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSampleSetWithComplementSet(df, nSamlpe):\n",
    "    colsName = df.columns\n",
    "    df = pd.DataFrame(df.values, index = range(len(df.index)))\n",
    "    df.columns = colsName\n",
    "    \n",
    "    df_sample = df.sample(nSamlpe)\n",
    "    list_comple = Series(df.index.isin(df_sample.index)).apply(lambda x: not x)\n",
    "    df_comple = df[list_comple]\n",
    "    return df_sample, df_comple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def selectSamplesByBagging(df, ratioOnPos, ratioP2N):\n",
    "    colsName = df.columns\n",
    "    df = pd.DataFrame(df.values, index = range(len(df.index)))\n",
    "    df.columns = colsName\n",
    "    \n",
    "    df_pos = df[df.label == 1]\n",
    "    list_pos = Series(df.index.isin(df_pos.index))\n",
    "    list_neg = list_pos.apply(lambda x: not x)\n",
    "    df_neg = df[list_neg]\n",
    " \n",
    "    len_pos = len(df_pos)  \n",
    "    len_bag = int(ratioOnPos * len_pos)\n",
    "\n",
    "    df_bag_pos, df_left_pos = getSampleSetWithComplementSet(df_pos, len_bag)\n",
    "    df_bag_neg, df_left_neg = getSampleSetWithComplementSet(df_neg, (int(len_bag / ratioP2N)))\n",
    "                                                            \n",
    "    df_bag = pd.concat([df_pos.sample(len_bag), df_neg.sample(int(len_bag / ratioP2N))], axis = 0)\n",
    "    df_left = pd.concat([df_left_pos, df_left_neg], axis = 0)\n",
    "    return df_bag, df_left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_F1(y ,y_pred):\n",
    "\n",
    "\ty = DataFrame(y, columns = ['label'])\n",
    "\ty_pred = DataFrame(y_pred, columns = ['label'])\n",
    "\n",
    "\tpos_yp = y_pred[y_pred.label == 1]\n",
    "\tneg_yp = y[y_pred.label == 0]\n",
    "\tpos_y = y[y.label == 1]\n",
    "\tneg_y = y[y.label == 0]\n",
    "\t\n",
    "\t## calc TP and TN\n",
    "\tlen_yp = len(y_pred); len_y = len(y)\n",
    "\t\n",
    "\tTP = sum(pos_yp.index.isin(pos_y.index))\n",
    "\tFP = len(pos_yp) - TP\n",
    "\tTN = sum(neg_yp.index.isin(neg_y.index))\n",
    "\tFN = len(neg_yp) - TN\n",
    "\n",
    "\t## calc R, P, F1\n",
    "\tif (len(pos_yp) == 0 or len(pos_y) == 0 or TP == 0):\n",
    "\t\tP = 0; R = 0; F1 = 0\n",
    "\telse:\n",
    "\t\tR = 1.0 * TP / len(pos_yp) * 100\n",
    "\t\tP = 1.0 * TP / len(pos_y) * 100\n",
    "\t\tF1 = (2.0 * R * P) / (R + P)\n",
    "\t\t# the two following formulation are also right\n",
    "\t# R = 1.0 * TP / (TP + FP) * 100\n",
    "\t# P = 1.0 * TP / (TP + FN) * 100 \n",
    "\treturn F1, P, R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-1fa0c46d23bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'2014-12-08'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPATH_OFFLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'statisFeat_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'4days.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mdf_easySet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_cart_h\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mdf_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_easySet\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    527\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 529\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    530\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    303\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 305\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m _parser_defaults = {\n",
      "\u001b[1;32mD:\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    761\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'skip_footer not supported for iteration'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 763\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    764\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    765\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'as_recarray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1211\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1212\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1213\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1214\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## 生成训练集\n",
    "date = '2014-11-22'\n",
    "df_train = DataFrame()\n",
    "while date != '2014-12-08':\n",
    "    df = pd.read_csv(PATH_OFFLINE + 'statisFeat_' + date + '_' + '4days.csv')\n",
    "    df_easySet = df[df.last_cart_h > 0]\n",
    "    df_train = pd.concat([df_train, df_easySet], axis = 0)\n",
    "    date = getOffsetDate(date, 1)\n",
    "print(df_train.iloc[:10])\n",
    "\n",
    "PATH_OF_OFFLINE = 'F:/codeGit/my project/python/dataset/tianchi/offline/'\n",
    "# df_train.to_csv(PATH_OF_OFFLINE + 'validTrainSet' + .csv', mode = 'w', index = False)\n",
    "df_train = pd.read_csv(PATH_OF_OFFLINE + 'validTrainSet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainig model...\n",
      "[8.83977900552486, 8.717203831425408, 8.818454459553838, 8.75292118121946, 8.669575786558273, 8.949853366334574]\n",
      "trainig model...\n",
      "[8.543804061006796, 8.529735071955143, 8.531786279362246, 8.636205235430374, 8.629379476401331, 8.634670617965556]\n"
     ]
    }
   ],
   "source": [
    "## 训练多个随机森林模型\n",
    "model = RandomForestClassifier(n_estimators=100, n_jobs=4, random_state=24)\n",
    "models_RF, paraList_RF = trainWithMutliTime(model, df_train, 10, 6)\n",
    "print(paraList_RF)\n",
    "## 训练多个gbdt模型\n",
    "model = GradientBoostingClassifier(max_depth=6)\n",
    "models_GBDT, paraList_GBDT = trainWithMutliTime(model, df_train, 10, 6)\n",
    "print(paraList_GBDT)\n",
    "models_RF.extend(models_GBDT)\n",
    "models = models_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 生成测试集\n",
    "date = '2014-12-08'\n",
    "df_test = DataFrame()\n",
    "while date != '2014-12-11':\n",
    "    df = pd.read_csv(PATH_OFFLINE + 'statisFeat_' + date + '_' + '4days.csv')\n",
    "    df_test = pd.concat([df_test, df], axis = 0)\n",
    "    date = getOffsetDate(date, 1)\n",
    "print(df)\n",
    "\n",
    "date = '2014-12-16'\n",
    "while date != '2014-12-19':\n",
    "    df = pd.read_csv(PATH_OFFLINE + 'statisFeat_' + date + '_' + '4days.csv')\n",
    "    df_test = pd.concat([df_test, df], axis = 0)\n",
    "    date = getOffsetDate(date, 1)\n",
    "print(df)\n",
    "df_test.to_csv(PATH_OF_OFFLINE + 'validTestSet' + '.csv', mode = 'w', index = False)\n",
    "df_test = df_test.drop(['user_id', 'item_id', 'item_category'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def trainStackingModel(models, stackModel, X, y): \n",
    "    X_layer1 = []\n",
    "    for i in range(len(models)):\n",
    "        X_layer1.append(models[i].predict(X).tolist())\n",
    "    X_layer1 = np.array(X_layer1).T\n",
    "    stackModel = stackModel.fit(X_layer1, y)\n",
    "    return stackModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predictStackingModel(models, stackModel, X):\n",
    "    X_layer1 = []\n",
    "    for i in range(len(models)):\n",
    "        X_layer1.append(models[i].predict(X))\n",
    "    X_layer1 = np.array(X_layer1).T        \n",
    "    y = stackModel.predict(X_layer1)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1/P/R 7.43%/23.26%/4.42%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 单模型测试\n",
    "\n",
    "df_test_1 = df_test[df_test.last_cart_h > 0]\n",
    "df_test_2 = df_test[df_test.last_cart_h <= 0]\n",
    "\n",
    "X_test_1, y_test_1, le = transform_dataset(df_test_1)\n",
    "X_test_2, y_test_2, le = transform_dataset(df_test_2)\n",
    "model = models[0]\n",
    "y_pred_1 = model.predict(X_test_1)\n",
    "\n",
    "y_test = np.zeros(len(y_test_1) + len(y_test_2))\n",
    "y_pred = np.zeros(len(y_test_1) + len(y_test_2))\n",
    "\n",
    "y_test[: len(y_test_1)] = y_test_1\n",
    "y_test[len(y_test_1):] = y_test_2\n",
    "\n",
    "y_pred[: len(y_pred_1)] = y_pred_1\n",
    "y_pred[len(y_pred_1):] = 0\n",
    "\n",
    "F1, P, R = calc_F1(y_test, y_pred)\n",
    "print 'F1/P/R %.2f%%/%.2f%%/%.2f%%\\n' %(F1, P, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9146L, 12L)\n",
      "F1/P/R 5.56%/88.62%/2.87%\n",
      "\n",
      "F1/P/R 7.34%/22.70%/4.37%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 直接单模型进行stacking\n",
    "df_bag, df_cv = selectSamplesByBagging(df_train, 0.6, 1)\n",
    "X_test, y_test, le = transform_dataset(df_cv.drop(['user_id', 'item_id', 'item_category'], axis = 1))\n",
    "X_train, y_train, le1 = transform_dataset(df_bag.drop(['user_id', 'item_id', 'item_category'], axis = 1))\n",
    "\n",
    "stackModel = RandomForestClassifier(n_estimators=100, n_jobs=4, random_state=24)\n",
    "stackModel = trainStackingModel(models, stackModel, X_train, y_train)\n",
    "y_pred = predictStackingModel(models, stackModel, X_test)\n",
    "pred_labels = le.inverse_transform(y_pred)\n",
    "F1, P ,R = calc_F1(y_test, pred_labels)\n",
    "print 'F1/P/R %.2f%%/%.2f%%/%.2f%%\\n' %(F1, P, R)\n",
    "\n",
    "df_test_1 = df_test[df_test.last_cart_h > 0]\n",
    "df_test_2 = df_test[df_test.last_cart_h <= 0]\n",
    "\n",
    "X_test_1, y_test_1, le = transform_dataset(df_test_1)\n",
    "X_test_2, y_test_2, le = transform_dataset(df_test_2)\n",
    "y_pred_1 = predictStackingModel(models, stackModel, X_test_1)\n",
    "\n",
    "y_test = np.zeros(len(y_test_1) + len(y_test_2))\n",
    "y_pred = np.zeros(len(y_test_1) + len(y_test_2))\n",
    "\n",
    "y_test[: len(y_test_1)] = y_test_1\n",
    "y_test[len(y_test_1):] = y_test_2\n",
    "\n",
    "y_pred[: len(y_pred_1)] = y_pred_1\n",
    "y_pred[len(y_pred_1):] = 0\n",
    "\n",
    "F1, P, R = calc_F1(y_test, y_pred)\n",
    "print 'F1/P/R %.2f%%/%.2f%%/%.2f%%\\n' %(F1, P, R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如上对单模型进行stacking的结果还比不上单模型直接预测的结果，这说明了stacking并不能有效减少方差。\n",
    "- 若使用类平衡化的训练集进行训练，训练结果的cv与test集的结果如下：\n",
    "\n",
    "F1/P/R 5.56%/88.62%/2.87%  \n",
    "F1/P/R 7.34%/22.70%/4.37%  \n",
    "\n",
    "- 若使用类不平衡的训练集进行训练，训练结果的cv与test集的结果如下：\n",
    "\n",
    "F1/P/R 5.51%/4.46%/7.21%  \n",
    "F1/P/R 0.30%/0.17%/1.27%  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def baggingModels(models, X):\n",
    "    y_pred = 0\n",
    "    nModel = len(models)\n",
    "    decs = nModel - 1\n",
    "    for i in range(nModel):\n",
    "        y_pred = y_pred + models[i].predict(X)\n",
    "    y_pred = DataFrame(y_pred, columns = ['label'])\n",
    "    # y_pred.label = y_pred.label.apply(lambda x: 1 if x > decs else 0)\n",
    "    return y_pred.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainig model...\n",
      "[8.641329497213446, 8.586366775180032, 8.923449682636653, 8.799493255521748, 8.598659923395916, 8.254351197656387, 8.421669679058182, 8.667421517249672, 8.694850304309515, 8.816912214492334, 8.715393578976977, 8.426737410583165, 8.910704172172927, 8.687780964585748, 8.56263666676561, 8.482033010836295, 8.609917804360292, 8.855336494468013, 8.375670943482879, 8.749795517749062, 8.402051439953045, 8.64275987290059, 8.74073924337218, 8.766836614314585, 8.648703506368296, 8.518591383041512, 8.8845241702101, 8.665677024697878, 8.531784322050253, 8.356851828663347, 8.695305332854693, 8.507644961164575, 8.85290825985545, 8.58985518714885, 8.989946839307333, 8.856925782916852, 8.372589781244637, 8.746149573296975, 8.542590099165595, 8.646730253473411]\n",
      "trainig model...\n",
      "[8.539765991443739, 8.35131614276196, 8.333497469027593, 8.517324738114423, 8.468554777159051, 8.356726408982942, 8.644654520817674, 8.724903433931832, 8.807133714841807, 8.200812813770023, 8.510850785471034, 8.299492385786802, 8.228421600570668, 8.516252118407566, 8.709356337093565, 8.51196649180922, 8.480586730067902, 8.544693170656052, 8.387538854186323, 8.598614850284635, 8.441833706776478, 8.528192414126556, 8.7327525509936, 8.235711070029405, 8.53325642002905, 8.442433383609856, 8.77820062656378, 8.490878273340888, 8.386416115932798, 8.545883496521906, 8.648280558392916, 8.551661338374592, 8.502335362031664, 8.210223804201844, 8.508319627054682, 8.640554212032567, 8.796603562887812, 8.799405037777944, 8.397490315466955, 8.660920313324572]\n"
     ]
    }
   ],
   "source": [
    "## 训练多个随机森林模型\n",
    "model = RandomForestClassifier(n_estimators=100, n_jobs=4, random_state=24)\n",
    "models_RF, paraList_RF = trainWithMutliTime(model, df_train, 40, 40)\n",
    "print(paraList_RF)\n",
    "## 训练多个gbdt模型\n",
    "model = GradientBoostingClassifier(max_depth=6)\n",
    "models_GBDT, paraList_GBDT = trainWithMutliTime(model, df_train, 40, 40)\n",
    "print(paraList_GBDT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3050.0 292181\n",
      "80 10\n",
      "10\n",
      "10\n",
      "F1/P/R 1.23%/1.47%/1.06%\n",
      "\n",
      "10\n",
      "10\n",
      "F1/P/R 1.65%/1.93%/1.44%\n",
      "\n",
      "10\n",
      "10\n",
      "F1/P/R 1.45%/1.69%/1.27%\n",
      "\n",
      "10\n",
      "10\n",
      "F1/P/R 1.33%/1.57%/1.16%\n",
      "\n",
      "10\n",
      "10\n",
      "F1/P/R 1.11%/1.49%/0.89%\n",
      "\n",
      "10\n",
      "10\n",
      "F1/P/R 1.00%/1.32%/0.80%\n",
      "\n",
      "10\n",
      "10\n",
      "F1/P/R 1.25%/1.69%/1.00%\n",
      "\n",
      "10\n",
      "10\n",
      "F1/P/R 1.44%/1.96%/1.14%\n",
      "\n",
      "F1/P/R 7.26%/22.31%/4.33%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 使用bagging模型再使用stacking模型\n",
    "df_bag, df_cv = selectSamplesByBagging(df_train, 0.6, 1)\n",
    "print sum(df_cv.label), len(df_cv)\n",
    "X_test, y_test, le = transform_dataset(df_cv.drop(['user_id', 'item_id', 'item_category'], axis = 1))\n",
    "X_train, y_train, le1 = transform_dataset(df_bag.drop(['user_id', 'item_id', 'item_category'], axis = 1))\n",
    "\n",
    "models = models_RF[:40]\n",
    "models.extend(models_GBDT[:40])\n",
    "X_layer1 = []\n",
    "nStackBaseModel = 8\n",
    "nBagModel = int(len(models) / nStackBaseModel)\n",
    "\n",
    "df_test_1 = df_test[df_test.last_cart_h > 0]\n",
    "X_test_1, y_test_1, le = transform_dataset(df_test_1)\n",
    "\n",
    "y_pred_layer1 = []\n",
    "for i in range(nStackBaseModel):\n",
    "    X_layer1.append(baggingModels(models[i * nBagModel : (i+1) * nBagModel], X_train).tolist())\n",
    "    y_pred = baggingModels(models[i * nBagModel : (i+1) * nBagModel], X_test_1)\n",
    "    test(df_test, y_pred)\n",
    "    \n",
    "    y_pred_layer1.append(y_pred)\n",
    "    # pred_labels = le.inverse_transform(y_pred)\n",
    "    # F1, P ,R = calc_F1(y_test, pred_labels)\n",
    "    # print 'F1/P/R %.2f%%/%.2f%%/%.2f%%\\n' %(F1, P, R)\n",
    "    \n",
    "X_layer1 = np.array(X_layer1).T\n",
    "stackModel = RandomForestClassifier(n_estimators=100, n_jobs=4, random_state=24)\n",
    "stackModel = stackModel.fit(X_layer1, y_train)\n",
    "\n",
    "## 测试\n",
    "X_layer1 = np.array(y_pred_layer1).T\n",
    "y_pred = stackModel.predict(X_layer1)\n",
    "test(df_test, y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(df_test, y_pred_1):\n",
    "    df_test_1 = df_test[df_test.last_cart_h > 0]\n",
    "    df_test_2 = df_test[df_test.last_cart_h <= 0]\n",
    "\n",
    "    X_test_1, y_test_1, le = transform_dataset(df_test_1)\n",
    "    # y_pred_1 = baggingModels(models, X_test_1)\n",
    "\n",
    "    X_test_2, y_test_2, le = transform_dataset(df_test_2)\n",
    "\n",
    "    y_test = np.zeros(len(y_test_1) + len(y_test_2))\n",
    "    y_pred = np.zeros(len(y_test_1) + len(y_test_2))\n",
    "\n",
    "    y_test[: len(y_test_1)] = y_test_1\n",
    "    y_test[len(y_test_1):] = y_test_2\n",
    "\n",
    "    y_pred[: len(y_pred_1)] = y_pred_1\n",
    "    y_pred[len(y_pred_1):] = 0\n",
    "\n",
    "    F1, P, R = calc_F1(y_test, y_pred)\n",
    "    print 'F1/P/R %.2f%%/%.2f%%/%.2f%%\\n' %(F1, P, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1831\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "         label\n",
      "1207         1\n",
      "1215         1\n",
      "1223         0\n",
      "2905         0\n",
      "3000         1\n",
      "3018         1\n",
      "3024         1\n",
      "3039         1\n",
      "3042         1\n",
      "5295         1\n",
      "5828         0\n",
      "5868         0\n",
      "10346        0\n",
      "10359        0\n",
      "10379        0\n",
      "10389        0\n",
      "10396        0\n",
      "10399        1\n",
      "10407        0\n",
      "12093        0\n",
      "12453        0\n",
      "13015        0\n",
      "13026        0\n",
      "13790        0\n",
      "13875        1\n",
      "17076        0\n",
      "17621        0\n",
      "17721        1\n",
      "17734        0\n",
      "17743        1\n",
      "...        ...\n",
      "1209604      0\n",
      "1209631      1\n",
      "1209653      0\n",
      "1209654      0\n",
      "1209663      0\n",
      "1209674      0\n",
      "1209679      0\n",
      "1209680      0\n",
      "1209699      0\n",
      "1209741      1\n",
      "1209751      0\n",
      "1212805      0\n",
      "1212809      0\n",
      "1212826      0\n",
      "1212847      0\n",
      "1212918      1\n",
      "1212931      0\n",
      "1212953      1\n",
      "1212956      1\n",
      "1212958      0\n",
      "1212987      1\n",
      "1213001      1\n",
      "1213005      0\n",
      "1213007      0\n",
      "1215015      1\n",
      "1215177      0\n",
      "1215215      0\n",
      "1215217      0\n",
      "1215440      1\n",
      "1215566      0\n",
      "\n",
      "[1831 rows x 1 columns]\n",
      "           user_id    item_id  label\n",
      "1207        161481   46870795      1\n",
      "1215        161481  209171499      1\n",
      "1223        161481  339030565      0\n",
      "2905        302466  390841005      0\n",
      "3000        317091  151712003      1\n",
      "3018        317091  233292622      1\n",
      "3024        317091  256677806      1\n",
      "3039        317091  326108663      1\n",
      "3042        317091  345845463      1\n",
      "5295        447828  206016051      1\n",
      "5828        530238   69382307      0\n",
      "5868        530238  165455096      0\n",
      "10346      1211491  136221535      0\n",
      "10359      1211491  170044680      0\n",
      "10379      1211491  242480594      0\n",
      "10389      1211491  298164939      0\n",
      "10396      1211491  331684471      0\n",
      "10399      1211491  349443097      1\n",
      "10407      1211491  378659296      0\n",
      "12093      1321831  264597138      0\n",
      "12453      1390654  268612485      0\n",
      "13015      1446949  107138545      0\n",
      "13026      1446949  215008237      0\n",
      "13790      1591612   36907872      0\n",
      "13875      1591612  353656592      1\n",
      "17076      1953838  243247546      0\n",
      "17621      2046058   54960667      0\n",
      "17721      2048794  199187082      1\n",
      "17734      2048794  252564781      0\n",
      "17743      2048794  349017358      1\n",
      "...            ...        ...    ...\n",
      "1209604  141816551   27293772      0\n",
      "1209631  141816551   79038740      1\n",
      "1209653  141816551  123366046      0\n",
      "1209654  141816551  124519748      0\n",
      "1209663  141816551  142221910      0\n",
      "1209674  141816551  173175079      0\n",
      "1209679  141816551  183655241      0\n",
      "1209680  141816551  184726470      0\n",
      "1209699  141816551  265538909      0\n",
      "1209741  141816551  385376939      1\n",
      "1209751  141816551  404043314      0\n",
      "1212805  142014633     928037      0\n",
      "1212809  142014633   13781372      0\n",
      "1212826  142014633   37683117      0\n",
      "1212847  142014633   77003388      0\n",
      "1212918  142014633  227433318      1\n",
      "1212931  142014633  254836176      0\n",
      "1212953  142014633  299363124      1\n",
      "1212956  142014633  302419754      1\n",
      "1212958  142014633  310969312      0\n",
      "1212987  142014633  365217992      1\n",
      "1213001  142014633  392406846      1\n",
      "1213005  142014633  397417162      0\n",
      "1213007  142014633  401037080      0\n",
      "1215015  142159824  179625206      1\n",
      "1215177  142191573    3892822      0\n",
      "1215215  142191573   44007043      0\n",
      "1215217  142191573   44523695      0\n",
      "1215440  142191573  277149740      1\n",
      "1215566  142191573  397670044      0\n",
      "\n",
      "[1831 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "## 提交\n",
    "df_sub = pd.read_csv(PATH_OFFLINE + 'statisFeat_' + '2014-12-19_' + '4days.csv')\n",
    "itemSet = pd.read_csv('F:/codeGit/dataset/tianchi_mobile/tianchi_fresh_comp_train_item.csv', usecols = ['item_id'])\n",
    "itemSet = itemSet['item_id'].apply(lambda x: str(x))\n",
    "df_sub = df_sub[df_sub['item_id'].isin(itemSet)]\n",
    "\n",
    "df_sub = df_sub[df_sub.last_cart_h > 0]\n",
    "uidList = df_sub[['user_id', 'item_id']]\n",
    "print len(df_sub)\n",
    "X_sub = df_sub.drop(['user_id', 'item_id', 'item_category'], axis = 1).values\n",
    "\n",
    "models = models_RF[:40]\n",
    "models.extend(models_GBDT[:40])\n",
    "nStackBaseModel = 8\n",
    "nBagModel = int(len(models) / nStackBaseModel)\n",
    "y_pred_layer1 = []\n",
    "for i in range(nStackBaseModel):\n",
    "    y_pred_layer1.append(baggingModels(models[i * nBagModel : (i+1) * nBagModel], X_sub).tolist())\n",
    "X_layer1 = np.array(y_pred_layer1).T\n",
    "y_sub_pred = stackModel.predict(X_layer1)\n",
    "\n",
    "predList = pd.DataFrame(y_sub_pred, index = uidList.index, columns = ['label'])\n",
    "print predList\n",
    "uidList['label'] = predList\n",
    "print uidList\n",
    "uidList = uidList[uidList['label'] == 1]\n",
    "\n",
    "uidList[['user_id', 'item_id']].to_csv(PATH_OF_DATAOUT + 'tianchi_mobile_recommendation_predict.csv', mode = 'w', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
